{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import math \n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import librosa\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "target_sampling_rate = 4000\n",
    "volunteer_id = 1 # For dataset with all volunteers use volunteer_id=5\n",
    "\n",
    "input_dir = \"../Dataset/Non-Interfered\"\n",
    "google_dir = \"../Dataset/Interference\"\n",
    "output_dir = \"../Dataset/Interfered/Volunteer_1\"\n",
    "isExist = os.path.exists(output_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arr = [\"backward\",\"bed\",\"bird\",\"cat\",\"dog\",\"down\",\"eight\",\"five\",\"follow\",\"forward\",\"four\",\"go\",\"happy\",\"house\",\"learn\",\"left\",\"marvin\",\"nine\",\"no\",\"off\",\"on\",\"one\",\"right\",\"seven\",\"sheila\",\"six\",\"stop\",\"three\",\"tree\",\"two\",\"up\",\"visual\",\"wow\",\"yes\",\"zero\",\"google\",\"siri\",\"bixby\",\"alexa\"]\n",
    "test_label_ids = [5, 11, 15, 19, 22, 26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'bird': 1,\n",
    " 'happy': 1,\n",
    " 'cat': 1,\n",
    " 'dog': 1,\n",
    " 'follow': 1,\n",
    " 'house': 1,\n",
    " 'forward': 1,\n",
    " 'bed': 1,\n",
    " 'backward': 1,\n",
    " 'sheila': 1,\n",
    " 'tree': 1,\n",
    " 'two': 1,\n",
    " 'down': 5,\n",
    " 'four': 1,\n",
    " 'eight': 1,\n",
    " 'visual': 1,\n",
    " 'five': 1,\n",
    " 'marvin': 1,\n",
    " 'go': 11,\n",
    " 'learn': 1,\n",
    " 'wow': 1,\n",
    " 'left': 6,\n",
    " 'one': 1,\n",
    " 'seven': 1,\n",
    " 'off': 9,\n",
    " 'nine': 1,\n",
    " 'right': 7,\n",
    " 'up': 4,\n",
    " 'stop': 10,\n",
    " 'zero': 1,\n",
    " 'three': 1,\n",
    " 'on': 8,\n",
    " 'yes': 2,\n",
    " 'six': 1,\n",
    " 'no': 3,\n",
    " '_silence_': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stft(x, fs, n_fft, hop_length):\n",
    "    c_stft = librosa.stft(x, n_fft=n_fft, hop_length=hop_length)\n",
    "    return c_stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if volunteer_id > 4:\n",
    "    path_dirs = []\n",
    "    for file in os.listdir(input_dir):\n",
    "        d = os.path.join(input_dir, file)\n",
    "        if  os.path.isdir(d):\n",
    "            if \".\" in d:\n",
    "                continue\n",
    "            path_dirs.append(d)\n",
    "else:\n",
    "    path_dirs = [os.path.join(input_dir,\"Volunteer\"+str(volunteer_id))]\n",
    "\n",
    "print(\"path dirs\", path_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension = \"general_aud.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_target_data():\n",
    "\n",
    "    all_train_mic_data = []\n",
    "    all_train_imu_data = []\n",
    "    all_test_mic_data = []\n",
    "    all_test_imu_data = []\n",
    "\n",
    "    all_train_mic_stft = []\n",
    "    all_train_imu_stft = []\n",
    "    all_test_mic_stft = []\n",
    "    all_test_imu_stft = []\n",
    "    \n",
    "    all_test_labels = []\n",
    "    all_train_labels = []\n",
    "\n",
    "\n",
    "    for fpath in path_dirs:\n",
    "        if \".ipynb_checkpoints\" in fpath:\n",
    "            continue\n",
    "        print(fpath)\n",
    "\n",
    "\n",
    "\n",
    "        train_mic_data = []\n",
    "        train_imu_data = []\n",
    "        test_mic_data = []\n",
    "        test_imu_data = []\n",
    "        train_label = []\n",
    "\n",
    "        train_mic_stft = []\n",
    "        train_imu_stft = []\n",
    "        test_mic_stft = []\n",
    "        test_imu_stft = []\n",
    "        test_label = []\n",
    "\n",
    "\n",
    "        gen = os.path.join(fpath, file_extension)\n",
    "        gen_data = loadmat(gen)['word'][:,0:5]\n",
    "        sample_count = gen_data.shape[0]//len(label_arr)\n",
    "        \n",
    "        \n",
    "        for word in range(0, len(label_arr)):\n",
    "            imu_data = []\n",
    "            mic_data = []\n",
    "            imu_stft = []\n",
    "            mic_stft = []\n",
    "            labels = []\n",
    "\n",
    "\n",
    "            \n",
    "            for ind in range(0, sample_count):\n",
    "                d = gen_data[word + len(label_arr)*ind, :]\n",
    "                resampled_mic = librosa.resample(d[0].reshape(-1), 44100, target_sampling_rate)\n",
    "                normalizedsound = minmax_scale(resampled_mic, feature_range=(-1,1))\n",
    "                mic_data.append(normalizedsound)\n",
    "\n",
    "                imu_t = d[3][0:400,1:4]\n",
    "                temp_imu = np.sqrt(imu_t[:,0]**2 + imu_t[:,1]**2 + imu_t[:,2]**2)\n",
    "                normalizedimu = minmax_scale(temp_imu, feature_range=(-1,1))\n",
    "\n",
    "                imu_data.append(normalizedimu)\n",
    "\n",
    "                imu_stft.append(get_stft(x=normalizedimu, fs=normalizedimu.shape[0], n_fft=40, hop_length=20))\n",
    "                mic_stft.append(get_stft(normalizedsound, fs=normalizedsound.shape[0], n_fft=400, hop_length=200))\n",
    "                labels.append(label_arr[word])\n",
    "\n",
    "            \n",
    "            mic_data = np.asarray(mic_data)\n",
    "            imu_data = np.asarray(imu_data)\n",
    "            mic_stft = np.asarray(mic_stft)\n",
    "            imu_stft = np.asarray(imu_stft)\n",
    "            labels = np.asarray(labels)\n",
    "            \n",
    "            if word in test_label_ids:\n",
    "                test_mic_data.append(mic_data)\n",
    "                test_imu_data.append(imu_data)\n",
    "                test_mic_stft.append(mic_stft)\n",
    "                test_imu_stft.append(imu_stft)\n",
    "                test_label.append(labels)\n",
    "            \n",
    "            else:\n",
    "\n",
    "                train_mic_data.append(mic_data)\n",
    "                train_imu_data.append(imu_data)\n",
    "                train_mic_stft.append(mic_stft)\n",
    "                train_imu_stft.append(imu_stft)\n",
    "                train_label.append(labels)\n",
    "\n",
    "\n",
    "        all_train_mic_stft.append(train_mic_stft)\n",
    "        all_train_imu_stft.append(train_imu_stft)\n",
    "        all_test_mic_stft.append(test_mic_stft)\n",
    "        all_test_imu_stft.append(test_imu_stft)\n",
    "\n",
    "        all_train_mic_data.append(train_mic_data)\n",
    "        all_train_imu_data.append(train_imu_data)\n",
    "        all_test_mic_data.append(test_mic_data)\n",
    "        all_test_imu_data.append(test_imu_data)\n",
    "        \n",
    "        all_train_labels.append(train_label)\n",
    "        all_test_labels.append(test_label)\n",
    "\n",
    "\n",
    "    all_train_mic_data = np.asarray(all_train_mic_data)\n",
    "    all_train_imu_data = np.asarray(all_train_imu_data)\n",
    "    all_test_mic_data = np.asarray(all_test_mic_data)\n",
    "    all_test_imu_data = np.asarray(all_test_imu_data)\n",
    "\n",
    "    all_train_mic_stft = np.asarray(all_train_mic_stft)\n",
    "    all_train_imu_stft = np.asarray(all_train_imu_stft)\n",
    "    all_test_mic_stft = np.asarray(all_test_mic_stft)\n",
    "    all_test_imu_stft = np.asarray(all_test_imu_stft)\n",
    "    \n",
    "    all_train_labels = np.asarray(all_train_labels)\n",
    "    all_test_labels = np.asarray(all_test_labels)\n",
    "\n",
    "    print(all_train_mic_data.shape, all_train_imu_data.shape, all_test_mic_data.shape, all_test_imu_data.shape) \n",
    "    print(all_train_mic_stft.shape, all_train_imu_stft.shape, all_test_mic_stft.shape, all_test_imu_stft.shape) \n",
    "    print(all_train_labels.shape, all_test_labels.shape)\n",
    "\n",
    "    return [all_train_mic_data, all_train_imu_data, all_train_mic_stft, all_train_imu_stft, all_train_labels], [all_test_mic_data, all_test_imu_data, all_test_mic_stft, all_test_imu_stft, all_test_labels]\n",
    "\n",
    "target_train, target_test = read_target_data()\n",
    "pickle.dump( target_train, open( os.path.join(output_dir, \"target_train.p\"), \"wb\" ) )\n",
    "pickle.dump( target_test, open( os.path.join(output_dir, \"target_test.p\"), \"wb\" ) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Noise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_noise_data():\n",
    "    all_train_noise_data = []\n",
    "    all_test_noise_data = []\n",
    "    all_train_labels = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    not_present_ind = []\n",
    "\n",
    "    for words in label_arr:\n",
    "        temp_dir = os.path.join(google_dir, words)\n",
    "        print(temp_dir)\n",
    "        if not os.path.exists(temp_dir):\n",
    "            not_present_ind.append(label_arr.index(words))\n",
    "            continue\n",
    "        labeled_data = []\n",
    "        labeled_stft = []\n",
    "        labels = []\n",
    "        count_file = 0\n",
    "        for file in os.listdir(temp_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                if count_file == 500:\n",
    "                    break\n",
    "                d = librosa.load(os.path.join(temp_dir, file))\n",
    "                if len(d[0]) != 22050:\n",
    "                    continue\n",
    "                resampled_mic = librosa.resample(d[0].reshape(-1), 22050, target_sampling_rate)\n",
    "                normalizedsound = minmax_scale(resampled_mic, feature_range=(-1,1))\n",
    "                labeled_data.append(normalizedsound)\n",
    "                labels.append(words)\n",
    "                count_file += 1\n",
    "        if label_arr.index(words) in test_label_ids:\n",
    "            all_test_noise_data.append(labeled_data)\n",
    "            all_test_labels.append(labels)\n",
    "        else:\n",
    "            all_train_noise_data.append(labeled_data)\n",
    "            all_train_labels.append(labels)\n",
    "            \n",
    "\n",
    "    all_train_noise_data = np.asarray(all_train_noise_data)\n",
    "    all_test_noise_data = np.asarray(all_test_noise_data)\n",
    "    \n",
    "    all_train_labels = np.asarray(all_train_labels)\n",
    "    all_test_labels = np.asarray(all_test_labels)\n",
    "\n",
    "    print(all_train_noise_data.shape, all_test_noise_data.shape)\n",
    "    \n",
    "    return [all_train_noise_data, not_present_ind, all_train_labels], [all_test_noise_data, not_present_ind, all_test_labels]\n",
    "\n",
    "noise_train, noise_test = read_noise_data()\n",
    "\n",
    "pickle.dump( noise_train, open( os.path.join(output_dir, \"noise_train.p\"), \"wb\" ) )\n",
    "pickle.dump( noise_test, open( os.path.join(output_dir, \"noise_test.p\"), \"wb\" ) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output1\n",
    "not_present_ind = noise_test[1]\n",
    "test_index = []\n",
    "for user in range(0, 1):\n",
    "    for word in range(0,6):\n",
    "        for i in range(0, 10):\n",
    "            noise_word = 0\n",
    "            for noise_word_ind in range(0, 4):\n",
    "                if noise_word_ind in not_present_ind:\n",
    "                    continue\n",
    "                if noise_word_ind == word:\n",
    "                    continue\n",
    "\n",
    "                for j in range(0, 500):\n",
    "                    test_index.append([[user, word, i],[noise_word, j]])\n",
    "                noise_word += 1\n",
    "    \n",
    "shuffled_test_index = sklearn.utils.shuffle(test_index)\n",
    "print(len(shuffled_test_index))\n",
    "pickle.dump( shuffled_test_index, open( os.path.join(output_dir, \"combination_index_test.p\"), \"wb\" ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_index[1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "not_present_ind = noise_train[1]\n",
    "train_index = []\n",
    "for user in range(0, 1):\n",
    "    for word in range(0,33):\n",
    "        for i in range(0, 10):\n",
    "            noise_word = 0\n",
    "            for noise_word_ind in range(0, 26):\n",
    "                if noise_word_ind in not_present_ind:\n",
    "                    continue\n",
    "                if noise_word_ind == word:\n",
    "                    continue\n",
    "\n",
    "                for j in range(0, 500):\n",
    "                    train_index.append([[user, word, i],[noise_word, j]])\n",
    "                noise_word += 1\n",
    "    \n",
    "shuffled_train_index = sklearn.utils.shuffle(train_index)\n",
    "print(len(shuffled_train_index))\n",
    "pickle.dump( shuffled_train_index, open( os.path.join(output_dir, \"combination_index_train.p\"), \"wb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_index[10050])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "IMuV_Clean_Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
