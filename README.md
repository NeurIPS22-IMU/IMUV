# Self-Supervised Speech Enhancement using Multi-Modal Data

This repository is the official implementation of **Self-Supervised Speech Enhancement using Multi-Modal Data** (IMUV).

IMUV project aims to decode speech signal with the aided from earphone IMUs, which has the advantage of being insensitive to environmental noise.
  
    
## Requirements

This repo only contains the clean data (**H<sub>T</sub>** and **L**). To generate the noisy data required by IMUV:

1. Download the external speech dataset from [Google Speech Command Dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) as interference
    - Run ```./install_download.sh```
2. Mix the external data with the clean data
    - Execute *Code/IMUV_Generate_Dataset.ipynb*

## Training and Evaluation

- To train and evaluate the model(s) in the paper:
    - Execute *Code/IMUV_train_test.ipynb*

## Pre-trained Models

- The pre-trained model is stored in *Code/models*. To test the pre-trained model:
    - Execute *Code/IMUV_test.ipynb*

## Results

 
Our model achieves the following performance (SIR = 5dB):

| Model | KWS 10 classes | KWS 35 classes |
| ---| --- | --- |
| Supervised IMUV | 91% | 86% |
| Self-Supervised IMUV | 83% | 72% |

## Directory Structure
    .
    ├── Dataset 
        ├── Non-Interfered                      # Target data H_T along with IMU signal L
            ├── Volunteer1                         
                ├── general_aud.mat             # data collected when users speak normal words
                ├── wake_aud.mat                # data collected when users speak wake words, 
            ├── Volunteer2                   
                ├── general_aud.mat     
                ├── wake_aud.mat
            ├── Volunteer3                   
                ├── general_aud.mat     
                ├── wake_aud.mat
            ├── Volunteer4                   
                ├── general_aud.mat     
                ├── wake_aud.mat
        ├── Interference                        # Empty, run ./install_download.sh to download the public interference dataset (Google’s speech command dataset)
        ├── Interfered                          # Empty, run Code/IMUV_Generate_Dataset.ipynb to synthesize the noisy data
            ├── Volunteer1-4                    # For the detail data structure, please refer to ./Code/IMUV_test.ipynb or ./Code/IMUV_Generate_Dataset.ipynb
    ├── Code
        ├── IMUV_Generate_Dataset.ipynb         # Code for generating dataset "Dataset/Interfered"
        ├── IMUV_train_test.ipynb               # Full code for training and testing
        ├── IMUV_test.ipynb                     # Testing with our pre-trained model
    ├── Examples                                # Contains sample spectrograms/checkpoints generated by IMUV
    
## Dataset Detail

IMUV dataset composes of speech data recorded using (1) IMU placed near the ear, (2) IMU placed near the neck, (3) contact microphone placed near the neck, and (4) normal microphone.

We recruited four volunteers, and each volunteer is asked to speak (1) a list of 39 normal words, (2) a list of voice assistant wake words (Alexa, Google, Siri, Bixby)
    
       
## Mat File Structure
Each row is a one-second speech/IMU clip<br />
col. 1: clean mic data (44.1kHz)<br />
col. 2: contact mic data (44.1kHz)<br />
col. 3: IMU data near the ear (~475Hz, microsecond timestamp in first column)<br />
col. 4: IMU data near the neck(~475Hz, microsecond timestamp in first column)<br />
col. 5 and 6: unused data at this moment<br />


## Contributing

Please send in fixes or feature additions through Pull Requests.

Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution, this simply gives us permission to use and redistribute your contributions as part of the project.
